# vjMyProfile


VIJAYA KUMAR CHOWDARY


Email: vijayk1930@gmail.com


Phone no: 2242146960



 
 

 
 




high
level EXPERIENCE SUMMARY


Overall 10+ years of IT experience in Business Analysis, Application
Design, Data Modelling, Implementation, Development and Testing of Data
Warehouse applications for Financial, Retail, Pharma and Taxation Domains.
Solid understanding
of Data Warehouse Architecture which
includes Star Schema, Snow Flake Schema, Facts &
Dimensions, representation of Physical and Logical data modeling using Erwin,
PowerDesigner.
Experience in
creating, and maintaining, conceptual, logical and physical data models
defining information requirements for management and business intelligence
purposes.
Experience in
training and mentoring client resources on ETL concepts and provide the
technical support in the process of job development.
Extensive knowledge of complete Software Development Life Cycle (SDLC).
Hands on expertise in
designing and developing DataStage Parallel
and Server jobs using IBM InfoSphere
DataStage versions 11.5, 9.1, 8.x, and 7.x.
Effectively used IBM
InfoSphere QualityStage components
Investigate, Standardize, Match , Survivorship and AVI stage to discover Data
Anomalies, Enforcing business standards, create consolidated data of customer,
get best-of-breed data, and to Validate the address information respectively.
Extensive experience
in extracting and loading complex XML files.
Wide range of experience in
executing Onshore-Offshore model.
Proficient in loading
various source systems into relational databases like Oracle, Netezza, Teradata, and DB2.
Expertise with Oracle SQL
Loader and Data Pump.
Proficient in SQL
Development and tuning query performance.
Experience in creating
Hadoop Cluster in Amazon Web Services.
Experience
in creating Dashboards and Reports using Data Analysis Tools like Tableau, SAP Business Objects, and MicroStrategy.
Experience in UNIX and Linux Shell scripts and Perl scripts.
Extensive experience with Python in creating wrapper scripts to
automate the deployment process and start and stop the clusters using Jenkins. 
Experience in reviewing Python code for running the
troubleshooting test-cases and bug issues. 
Wrote python scripts to parse XML documents and load the data in
database.
Hands on experience in coding MapReduce/Yarn Programs using Python for
analyzing Big Data.
Good understanding of XML methodologies (XML, XSL, XSD) including Web
Services and SOAP.
Extensive experience in writing PL/SQL Stored Procedures, Packages,
Views, Functions, and Triggers.
Good exposure to Oracle Administration tasks.
Experience in end-to-end
Hadoop Cluster setup in an internal Hadoop
Distribution called Invariant.
Experience in implementation
and ongoing administration of Hadoop infrastructure.
Experience in working with AGILE and SCRUM methodologies.
Knowledge of NO SQL
concepts and CASSANDRA, MONGO DB, and HBase databases.




















































EDUCATION


Masters of Computer Application(M.C.A), India


Bachelor of Science (B.Sc.),
India


 


CERTIFICATIONS
& TRAINING


IBM Certified Solution Developer - InfoSphere DataStage
IBM Certified Solution Developer - InfoSphere QualityStage
Netezza Bronze Certificate
Got training on Cognos BI 








Got training on
AbInitio
 
 






TECHNICAL
EXPERTISE



 
  
  ETL Tool

  
  
  IBM
  InfoSphere DataStage 7.5, 8.x, 9.1. 11.5 (Designer, Director, Administrator
  ), QualityStage, AbInitio

  
 
 
  
  BI Reporting Tool

  
  
  SAP Business Objects, Cognos, MicroStrategy, Tableau

  
 
 
  
  Data Warehousing

  
  
  Data
  Marts, OLTP, OLAP, Normalization, Dimensional Modeling, Star Schema, SnowFlake
  Schema

  
 
 
  
  Databases

  
  
  Oracle, Teradata,
  Netezza, SQL Server, DB2

  
 
 
  
  Languages

  
  
  C++, SQL, PL/SQL,
  XML, Python, Perl

  
 
 
  
  Operating Systems

  
  
  Windows 98/XP/7/8, IBM
  AIX, RedHat Linux

  
 
 
  
  Data Modeling Tools

  
  
  Erwin, Power
  Designer

  
 
 
  
  Scheduling Tools

  
  
  ESP, UNIX Cron, CA7, Tidal, TWS, DataStage Scheduler

  
 
 
  
  Software Packages

  
  
  TOAD, Oracle SQL
  Developer, SQuirreL, ClearCase, ClearQuest

  
 
 
  
  Big Data

  
  
  Hadoop, Hive, Pig,
  MapReduce, Ambari, Kerberos Authentication

  
 


 


PROJECT
SUMMARY


Project: Enterprise
Data Revenue (EDR)


Role: ETL
Technical Lead / Hadoop Administrator


Client: State of California
Franchise Tax Board


Location: Sacramento, CA


Duration: April 2014 - Till Date


Roles &
Responsibilities                                                                                     



Involved in defining
ETL system architecture, implemented end to end ETL and Reporting solutions,
designed data & process flow, maintained data dictionary and business
definitions.
As a Technical Lead,
was responsible for designing the ETL Strategy & Architecture of the
Project and played a role of SCRUM Master for sprint planning, retro, backlog
grooming, daily stand-ups, and sprint demo.
Involved in analysis
of source systems, business requirements and identification of business rules.
Analyze existing ETL
architecture, identify pain points and propose scalable solutions.
Work with Data Warehouse
Architect and DBA team to optimize performance, analyze requirements and
come-up with an optimum data model based on the requirements.
Provided the
estimation for the DataStage jobs based on the job complexity.
Data Stage job
design, development & debugging to populate data to the target tables.
Used DB2 Connector,
ODBC Connector to Extract and Upsert the data.
Developed ETL test
plans based on test strategy. Created and executed test cases and test scripts
based on test strategy and test plans based on ETL Mapping document.
Worked with complex
XML files to extract and load into EDR tables.
Created SQL Scripts
required for extracting data from various tables and performed query performance tuning.
Created wrapper
scripts using Python to automate the process of deployment and start and stop
the clusters.
Reviewing Python code for running the troubleshooting test-cases and bug
issues. 
Build Back-end support for Application from ground up using Python, Shell
scripts & Perl.
Wrote python scripts to parse XML documents and load the data in
database. 
Troubleshooting
development and operational problems and provide timely solutions.
Experience in setup
in-house Hadoop Distribution called Invariant.
Responsible for
implementation and ongoing administration of Hadoop infrastructure.
Working with data
delivery teams to setup new Hadoop users. 
Monitor Hadoop
cluster connectivity and security.
Collaborating with
application teams to install operating system and Hadoop updates, patches,
version upgrades when required.
Diligently teaming
with the infrastructure, network, database, application and business
intelligence teams to guarantee high data quality and availability.
Environment: IBM InfoSphere DataStage v8.7/11.5, QualityStage, IBM DB2 v9.7, Unix AIX
v6.1, IBM Rational ClearCase, IBM Rational ClearQuest, IBM Tivoli Work
Scheduler, Hadoop, Hive, Pig, Ambari
















































 





Project: Field Service Optimization (FSO)


Role: DataStage Technical
Lead


Client: Hallmark
Cards Inc.


Location: Kansas
City, MO


Duration: April 2013 – April 2014


Roles &
Responsibilities                                                                                             



Involved in defining
ETL system architecture, implemented end to end ETL and Reporting solutions,
designed data & process flow, maintained data dictionary and business
definitions.
Provided the
estimation for the DataStage jobs based on the job complexity.
As a Lead responsible
for planning the project timelines and getting the timely status from team
members to make sure the tasks completed in the given timeframe.
Coordinating with
offshore team to explain them the project requirements and to get the daily
status updates.
Mentoring Client
Associates in DataStage development, as they are new to DataStage.
Understanding the impact
of changes to be made on existing system.
Perform an impact
analysis for changes on the existing system.
Data Stage job
design, development & debugging to populate data to the target tables.
Extensively used SAP
IDOC, ABAP Extract stages to fetch and load data to and from SAP System.
Used Teradata Connector, ODBC Connector to
Extract and Upsert the data.
Designed Sequencer to
automate the whole process of data loading.
Developed ETL test
plans based on test strategy. Created and executed test cases and test scripts
based on test strategy and test plans based on ETL Mapping document.
Created SQL Scripts
required for extracting data from various tables.
Created Unix Scripts
to execute the DataStage jobs from Job Schedulers.
Used CA7 Scheduler
for scheduling DataStage jobs using JCL scripts.
Involved in UAT.
Checking the coding
standards.
Documenting the
changes and uploading in CVS. 
Environment: IBM InfoSphere DataStage v8.7, Teradata, SQL Server, Unix, CA-7, Tableau








































 





Project: Operational
Data Store (ODS), Billing and Payment (BNP)


Role: DataStage
Designer, Project Lead


Client: Walgreens/Catalyst
Rx/Catamaran Rx


Location: Deerfield,
IL


Duration: October 2010 – March 2013


Roles &
Responsibilities


Worked efficiently in
a multi-vendor environment and communicated effectively with offshore teams.
Developed an
understanding of client’s business process and requirements.
Developed high level
design documents for system architecture and process flow design, serving as a
fully proficient technical resource.
Provided accurate and
comprehensive Level of Effort estimates and scope documents.
Created generic
(multi instance) jobs to Extract, Transform, and Load the data.
Created data quality
jobs to do the field validations and data validations.
Processed Flat Files
by using Schema files to send the column definition in run time (RCP).
Created most critical
data transformation CDC logic to prepare load ready files.
Created Oracle SQL
Scripts required for extracting data from various tables.
Used Oracle SQL
Loader to load external flat files into target database.
Designed Sequencer to
automate the whole process of data loading.
Created Unix Scripts
to execute the DataStage jobs from Job Schedulers.
Used ESP and Tidal
for Scheduling DataStage jobs.
Worked in set up
Tidal to schedule the DataStage Sequencers and Unix Scripts.
24*7 On-Call Support.
Created Purge process
to delete the inactive claims from the BNP system.
Provided on-call support
and problem resolution for Reporting and ETL applications.
Apart from ETL, also
worked in Business Objects XI R3, and MicroStrategy to create WebI reports.
Strong abilities to
develop the Reports in BOBJ and MSTR according to the user requirements.
Environment: Data Stage 8.0v, SQL Developer, CSV files, flat files, IBM AIX, Oracle
10g, Toad 9.0, Business Objects XI R3, MicroStrategy, Tidal










































 





Project: One Customer Profile


Role: DataStage Developer


Client: Sears
Holding Corporation


Location:
Noida, India


Duration: April 2008 – October 2010


Roles &
Responsibilities


Analyze and
understand project requirements from business and technical perspective.
Coordinating with
Onshore team members to get the project requirements.
Mentored junior
developers.
Designing conceptual
data model, Development of data flow diagrams, data mapping documents, process
flows.
Designing and
Development of ETL jobs using DataStage v8.0.
Converted complex
BTEQ queries into DataStage jobs.
Created BTEQ scripts
to extract data from Teradata database.
Prepared nzsql
scripts for the Aggregate Tables.
Created Unix Scripts for
DataStage jobs.
Designed Sequencer to
automate the whole process of data loading.
Involved in preparing
LLDs, HLDs and Code reviewing.
Involved in Unit
testing.
Review of the project
deliverables.
Environment:   DataStage 8.0, Teradata, Netezza, Teradata
SQL Assistant, Unix






























 





Project: Insurance Information System


Role: DataStage Developer


Client: Tower
Insurance Corporation


Location: Hyderabad,
India


Duration: December 2006 – March 2008                                                



Roles &
Responsibilities


Used Data Stage
Designer in the development process for extracting, cleansing, transforming,
integrating, and loading data into target data marts. 
Involved in using
DataStage Director to compile, schedule, run and monitor DataStage jobs.
Used Data Stage
Manager involving functions like importing metadata into the DataStage
repository, creating new table definitions and job categories.
Involved in Using
Link Collector and Link Partitioner for performing data collection and data
partitioning Operations.
Implementing
performance-tuning techniques along various stages of the ETL process.
Understanding the
Requirement specifications.
Passed values by
loading parameters through Job properties.
Used DataStage
Manager to import and export metadata, job categories and data elements.
Designed Sequencer to
automate the whole process of data loading.
Involved in Unit
testing.  
Review of the project
deliverables 
Analyzing and
resolving the issues during UAT
Environment:   DataStage 7.5, Oracle, Toad, UNIX
